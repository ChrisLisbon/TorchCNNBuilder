{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f53ee3-145e-4cc9-9863-0a9675e74244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:39.651860Z",
     "start_time": "2024-02-20T15:12:39.635919Z"
    }
   },
   "outputs": [],
   "source": [
    "# setuping an absolute path to the project \n",
    "# not needed in case of `pip install`\n",
    "%run -i tools/setup_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afc6e9-fc1d-40a4-bcfd-5e4be2abe2a7",
   "metadata": {},
   "source": [
    "## Usage examples of `torchcnnbuilder.builder`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24142880-d5eb-43b4-a5e1-4a772eada5b9",
   "metadata": {},
   "source": [
    "This submodule contains functions and classes for an automation of the creation of CNN-models. So far, it is possible to calculate the sizes of tensors after convolutional layers and you can find there `EncoderBuilder` class, which creates convolutional and transposed convolutional sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b44305-3463-417b-92a9-5cd4940690fa",
   "metadata": {},
   "source": [
    "#### Function `convNd_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0440f252-6637-4848-b4f2-92c136c13f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:41.600544Z",
     "start_time": "2024-02-20T15:12:40.838986Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchcnnbuilder.builder import conv1d_out, conv2d_out, conv3d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61642af-4349-4f71-8304-49f666565bac",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **input_size**: size of the input tensor/vector\n",
    "- **kernel_size**: size of the convolution kernel. Default: 3\n",
    "- **stride**: stride of the convolution. Default: 1\n",
    "- **padding**: padding added to all four sides of the input. Default: 0\n",
    "- **dilation**: spacing between kernel elements. Default: 1\n",
    "- **n_layers**: number of conv layers\n",
    "\n",
    "Returns: one tuple as a size of the output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6bbfe-e2e5-469b-aa2b-8d11a84cf405",
   "metadata": {},
   "source": [
    "The size calculation after the convolutional layer is carried out according to the formula from the `torch` module *(the default parameters are the same as in `nn.ConvNd`)*. Counting functions are implemented for convolutions of dimensions from 1 to 3. At the same time, depending on the dimension, one number or the corresponding tuple of dimensions can be supplied to the parameters of each function. If it is necessary to calculate the convolution for tensors of N dimensions, then it is enough to simply apply a one-dimensional convolution N times. Some result values **can be negative** (due to the formula) which means you **should choose another conv params** (tensor dimensions degenerates to zero). The formula for calculating the size of the tensor after convolution for one dimension is presented below:\n",
    "\n",
    "$$\n",
    "H_{out} = \\lfloor \\frac{H_{in} + 2 \\times padding[0] - dilation[0] \\times (kernel[0] - 1) + 1}{stride[0]} \\rfloor + 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec60ce61-b194-44f9-803b-81357bf1fbd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:41.728912Z",
     "start_time": "2024-02-20T15:12:41.723719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.Conv1d: (15,)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv1d_out(input_size=33, \n",
    "                      kernel_size=5,\n",
    "                      stride=2)\n",
    "\n",
    "print(f'Tensor size after nn.Conv1d: {new_size}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb2a410-8a1e-4ad4-bbbb-f7663e5dda4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:42.068988Z",
     "start_time": "2024-02-20T15:12:42.063002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.Conv2d: (51, 32)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv2d_out(input_size=(55, 40), \n",
    "                      kernel_size=(4, 5),\n",
    "                      padding=(1, 0),\n",
    "                      dilation=(2, 2))\n",
    "\n",
    "print(f'Tensor size after nn.Conv2d: {new_size}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.Conv1d: (25, 25, 25)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv3d_out(input_size=33, \n",
    "                      n_layers=4)\n",
    "\n",
    "print(f'Tensor size after nn.Conv1d: {new_size}') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:42.424266Z",
     "start_time": "2024-02-20T15:12:42.408172Z"
    }
   },
   "id": "1198cecc166fc544"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.Conv1d: (39, 8)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv2d_out(input_size=(55, 40), \n",
    "                      kernel_size=(4, 5),\n",
    "                      padding=(1, 0),\n",
    "                      dilation=(2, 2),\n",
    "                      n_layers=4)\n",
    "\n",
    "print(f'Tensor size after nn.Conv1d: {new_size}') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:42.671976Z",
     "start_time": "2024-02-20T15:12:42.665931Z"
    }
   },
   "id": "c896ae9e0329848f"
  },
  {
   "cell_type": "markdown",
   "id": "8829e2c2-0a9c-4665-a762-18b312e1e159",
   "metadata": {},
   "source": [
    "#### Function `conv_transposeNd_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e5641-575d-4720-b7cb-a0c2047fc95b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:43.376810Z",
     "start_time": "2024-02-20T15:12:43.368480Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchcnnbuilder.builder import conv_transpose1d_out, conv_transpose2d_out, conv_transpose3d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24925b20-5892-44d2-934c-be80b29c0687",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **input_size**: size of the input tensor/vector\n",
    "- **kernel_size**: size of the convolution kernel. Default: 3\n",
    "- **stride**: stride of the convolution. Default: 1\n",
    "- **padding**: padding added to all four sides of the input. Default: 0\n",
    "- **output_padding**: controls the additional size added to one side of the output shape. Default: 0\n",
    "- **dilation**: spacing between kernel elements. Default: 1\n",
    "- **n_layers**: number of conv layers\n",
    "\n",
    "Returns: one tuple as a size of the output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c175060-2444-4832-99e4-70a264eba4cb",
   "metadata": {},
   "source": [
    "The size calculation after the transposed convolutional layer is carried out according to the formula from the torch module *(the default parameters are the same as in `nn.ConvTransposeNd`)*. Counting functions are implemented for transposed convolutions of dimensions from 1 to 3. At the same time, depending on the dimension, one number or the corresponding tuple of dimensions can be supplied to the parameters of each function. If it is necessary to calculate the transposed convolution for tensors of N dimensions, then it is enough to simply apply a one-dimensional transposed convolution N times. Some result values **can be negative** (due to the formula) which means you **should choose another conv params** (tensor dimensions degenerates to zero). The formula for calculating the size of the tensor after transposed convolution for one dimension is presented below:\n",
    "\n",
    "$$\n",
    "H_{out} = (H_{in} - 1) \\times stride[0] - 2 \\times padding[0] + dilation[0] \\times (kernel\\_size[0] - 1) + output\\_padding[0] + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08483cc3-d15e-452f-8753-097fb1b30bfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:45.868451Z",
     "start_time": "2024-02-20T15:12:45.850308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.ConvTranspose1d: (33,)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv_transpose1d_out(input_size=15, \n",
    "                                kernel_size=5,\n",
    "                                stride=2)\n",
    "\n",
    "print(f'Tensor size after nn.ConvTranspose1d: {new_size}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c94b53bb-5710-429e-bd55-17c6c3b7db4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T15:12:47.537724Z",
     "start_time": "2024-02-20T15:12:47.523445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.ConvTranspose2d: (55, 40)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv_transpose2d_out(input_size=(51, 32), \n",
    "                                kernel_size=(4, 5),\n",
    "                                padding=(1, 0),\n",
    "                                dilation=(2, 2))\n",
    "\n",
    "print(f'Tensor size after nn.ConvTranspose2d: {new_size}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor size after nn.ConvTranspose2d: (23, 23, 24)\n"
     ]
    }
   ],
   "source": [
    "new_size = conv_transpose3d_out(input_size=(11, 11, 12), \n",
    "                                kernel_size=3,\n",
    "                                dilation=(2, 2, 1),\n",
    "                                n_layers=3)\n",
    "\n",
    "print(f'Tensor size after nn.ConvTranspose2d: {new_size}') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T15:13:03.292978Z",
     "start_time": "2024-02-20T15:13:03.276852Z"
    }
   },
   "id": "6724711bffc7bbce"
  },
  {
   "cell_type": "markdown",
   "id": "bf86c1fb-a42f-4c47-b95e-639824d77773",
   "metadata": {},
   "source": [
    "### Class `Builder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6276f3-326b-4f70-859b-5c3ea618fb31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:53:56.957287Z",
     "start_time": "2024-02-09T14:53:56.937453Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchcnnbuilder.builder import Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4981e-452e-4a48-9047-8f25886c7954",
   "metadata": {},
   "source": [
    "Initialization params:\n",
    "\n",
    "- **input_size** (Sequence[int]): input size of the input tensor\n",
    "- **minimum_feature_map_size** (Union[Sequence[int], int]): minimum feature map size. Default: 5\n",
    "- **max_channels** (int): maximum number of layers after any convolution. Default: 512\n",
    "- **min_channels** (int): minimum number of layers after any convolution. Default: 32\n",
    "- **activation_function** (nn.Module): activation function. Default: nn.ReLU(inplace=True)\n",
    "- **finish_activation_function** (Union[str, Optional[nn.Module]]): last activation function, can be same as activation_function (str `'same'`). Default: None\n",
    "- **default_convolve_params** (dict[str, Union[int, tuple]]): parameters of convolutional layers (by default same as in torch)\n",
    "- **default_transpose_params** (dict[str, Union[int, tuple]]): parameters of transpose convolutional layers (by default same as in torch)\n",
    "\n",
    "Other attributes:\n",
    "- **conv_channels** (List[int]): list of output channels after each convolutional layer\n",
    "- **transpose_conv_channels** (List[int]): list of output channels after each transposed convolutional layer\n",
    "- **conv_layers** (List[tuple]): list of output tensor sizes after each convolutional layer\n",
    "- **transpose_conv_layers** (List[tuple]): list of output tensor sizes after each transposed convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090bc04-841e-4311-bc7c-508dd6a60354",
   "metadata": {},
   "source": [
    "Suppose we create a CNN-model for a tensor of size 100 by 100, let the smallest feature map after the convolutions be of size 3 by 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0c07ef-28ab-4131-b311-1912db349b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:53:58.615899Z",
     "start_time": "2024-02-09T14:53:58.592363Z"
    }
   },
   "outputs": [],
   "source": [
    "builder = Builder(input_size=[125, 125], \n",
    "                  minimum_feature_map_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b28373-e720-4c3c-93cf-72fe0e909fb2",
   "metadata": {},
   "source": [
    "#### Method `build_convolve_sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60624ac-7d3f-4e9a-835a-97eb1ecc3b5d",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **n_layers**: number of the convolution layers in the encoder part\n",
    "- **in_channels**: number of channels in the first input tensor. Default: 1\n",
    "- **params**: convolutional layer parameters (nn.Conv2d). Default: None\n",
    "- **normalization**: choice of normalization between str `'dropout'` and `'batchnorm'`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for dropout). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for dropout). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for batchnorm). Default: 1e-5\n",
    "- **momentum**: used for the running_mean -_var computation. Can be None for cumulative moving average (for batchnorm). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for batchnorm). Default: True\n",
    "- **ratio**: multiplier for the geometric progression of increasing channels (feature maps). Default: 2 (powers of two)\n",
    "- **start**: start position of a geometric progression in the case of `ascending=False`. Default: 32\n",
    "- **ascending**: the way of calculating the number of feature maps (with using `'ratio'` if False). Default: False\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential convolutional sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c45e9f-69f1-4553-bcdc-f276c06c91f7",
   "metadata": {},
   "source": [
    "The method helps to build convolutional sequences without writing the entire code by hand. The activation function is the one that was given at the time of initialization of the entire class. It is possible to use different types of normalization, but to manipulate the hyperparameters of this layer, it is necessary to build a convolutional sequence of convolutional blocks separately *(method `build_convolve_block`)*. The number of channels (feature maps) after each layer is calculated depending on the parameter `ascending`\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "    channel_i = start \\times ratio^{i}, \\quad i={1...n} & \\text{if } ascending = \\text{False} \\\\\n",
    "    stop = \\lfloor \\displaystyle\\frac{(input\\_size[0] + input\\_size[1]) * 0.5}{2} \\rfloor + in\\_channels & \\text{if } ascending = \\text{True} \\\\\n",
    "    step = \\displaystyle\\frac{stop - in\\_channels}{n\\_layers} \\\\ \n",
    "    channels = \\text{range}(in\\_channels, stop, step)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "998c5cd2-f504-4c93-9a7a-a4846d202ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:03.064240Z",
     "start_time": "2024-02-09T14:54:03.040477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd48a6-0fa5-4ecb-af55-6708a0e2f641",
   "metadata": {},
   "source": [
    "Sometimes the convolutional layer parameters you select can cause the tensor to degenerate after one of the layers or become smaller than the specified minimum size of the feature map. In this case, you will receive the corresponding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a40159c-46e4-47df-92d8-ad8c315b45f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:05.608771Z",
     "start_time": "2024-02-09T14:54:05.309013Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size and parameters can not provide more than 4 layers",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkernel_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpadding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdilation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/TorchCNNBuilder/torchcnnbuilder/builder.py:431\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, ascending, conv_dim)\u001B[0m\n\u001B[1;32m    424\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    425\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe difference in dimensions between input_size (input_size.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    426\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand convolution (conv_dim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconv_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) should not be more than 1 \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(input_size.shape - conv_dim should be equal to 1 or 0)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(torch\u001B[38;5;241m.\u001B[39mtensor(input_layer_size) \u001B[38;5;241m<\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminimum_feature_map_size)[\n\u001B[1;32m    430\u001B[0m                                           :\u001B[38;5;28mlen\u001B[39m(input_layer_size)]):\n\u001B[0;32m--> 431\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput size and parameters can not provide more than \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels:\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere is too many channels. Max channels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Input size and parameters can not provide more than 4 layers"
     ]
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=5, \n",
    "                                params={'kernel_size': 25, 'padding': 1, 'dilation': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e3dfb-8d11-4b32-9cb2-6abed6415ef3",
   "metadata": {},
   "source": [
    "Normalization layers come with standard parameters as in `torch`. To change the parameters of convolutional layers, pass them as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1736f784-9c7b-4cdd-a767-2b67b1599975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:08.896723Z",
     "start_time": "2024-02-09T14:54:08.878651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=2, \n",
    "                                in_channels=3,\n",
    "                                params={'kernel_size': 5, 'padding': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa31a8f6-6c93-4feb-86db-4b86e35bb219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:10.317144Z",
     "start_time": "2024-02-09T14:54:10.300858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                normalization='batchnorm',\n",
    "                                eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e256bf91-e3ab-4c2e-97a4-16c9f24105b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:10.675315Z",
     "start_time": "2024-02-09T14:54:10.651145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.1, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.1, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                normalization='dropout',\n",
    "                                p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7003c-07f2-4424-bc16-62a31e9c01d0",
   "metadata": {},
   "source": [
    "You can build a convolutional sequence from convolutional blocks. Each convolutional block is convolutional layers one after another, passing through which the tensor does not change its size. While this is an early functional, there is no point in making a convolutional sequence of such blocks, since the tensor will not change its size after going through the entire sequence, for this it is necessary to add the implementation of pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e967239b-ab3e-4e32-86da-df2213623ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:11.541267Z",
     "start_time": "2024-02-09T14:54:11.505314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (sub-block 1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Dropout2d(p=0.5, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (sub-block 2): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Dropout2d(p=0.5, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (sub-block 1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Dropout2d(p=0.5, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (sub-block 2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Dropout2d(p=0.5, inplace=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                sub_blocks=2,\n",
    "                                normalization='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846a107-26fc-48de-aa1a-c96f8e574d68",
   "metadata": {},
   "source": [
    "An example of using channel calculation using the `ascending=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516254e8-b799-4239-b73c-804178854376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:12.315658Z",
     "start_time": "2024-02-09T14:54:12.295251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (sub-block 1): Sequential(\n",
       "      (0): Conv2d(3, 34, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (sub-block 2): Sequential(\n",
       "      (0): Conv2d(34, 34, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (sub-block 1): Sequential(\n",
       "      (0): Conv2d(34, 65, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (sub-block 2): Sequential(\n",
       "      (0): Conv2d(65, 65, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                params={'kernel_size': 9},\n",
    "                                sub_blocks=2, \n",
    "                                ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a6696-f599-4bc3-88cd-3cbf5e3c66b7",
   "metadata": {},
   "source": [
    "To prevent the accidental creation of heavy models, by default there is a certain limit on the number of output channels (feature maps) after the convolutional layer. You can set it when initializing the class (or change it later as an attribute of the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bbdacb-bbe7-4a23-95e6-62dff9917eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:13.121414Z",
     "start_time": "2024-02-09T14:54:13.028170Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There is too many channels. Max channels 512 [layer 4]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkernel_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m9\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mascending\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# by default\u001B[39;49;00m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/TorchCNNBuilder/torchcnnbuilder/builder.py:434\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, ascending, conv_dim)\u001B[0m\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput size and parameters can not provide more than \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels:\n\u001B[0;32m--> 434\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere is too many channels. Max channels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_channels \u001B[38;5;129;01mand\u001B[39;00m layer \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ascending:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere is too few channels. Min channels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_channels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: There is too many channels. Max channels 512 [layer 4]"
     ]
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=5,\n",
    "                                params={'kernel_size': 9},\n",
    "                                in_channels=3,\n",
    "                                ascending=False, # by default\n",
    "                                ratio=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe0685-8a29-4a6f-977f-740ad60ac75b",
   "metadata": {},
   "source": [
    "If you reduce the number of layers in the previous case, then there will be no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee73502e-8fe3-4a92-a7b1-af712fbefae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:34.610646Z",
     "start_time": "2024-02-09T14:54:34.583108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 3): Sequential(\n",
       "    (0): Conv2d(96, 288, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_sequence(n_layers=3,\n",
    "                                in_channels=3,\n",
    "                                ascending=False, # by default\n",
    "                                ratio=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585571f43caa6bef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Also, you can do the same by using 1d or 3d convolution layers, however `Builder` should get the same number of dimensions in the `input_size` *(or not less than `len(input_size) - conv_dim`)*. All normalization layers would be replaced with the corresponding N-dimensional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e3feb1aecf3438e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:36.767949Z",
     "start_time": "2024-02-09T14:54:36.755904Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
       "    (1): Dropout1d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
       "    (1): Dropout1d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 3): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
       "    (1): Dropout1d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 4): Sequential(\n",
       "    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
       "    (1): Dropout1d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 5): Sequential(\n",
       "    (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "    (1): Dropout1d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_builder = Builder(input_size=(31,))\n",
    "test_builder.build_convolve_sequence(n_layers=5,\n",
    "                                    in_channels=1,\n",
    "                                    normalization='dropout',\n",
    "                                    conv_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9b1ace9fc69b188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:37.815953Z",
     "start_time": "2024-02-09T14:54:37.800529Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The difference in dimensions between input_size (input_size.shape=(31,)) and convolution (conv_dim=3) should not be more than 1 (input_size.shape - conv_dim should be equal to 1 or 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m error_test_builder \u001B[38;5;241m=\u001B[39m Builder(input_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m31\u001B[39m,))\n\u001B[0;32m----> 2\u001B[0m \u001B[43merror_test_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatchnorm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43mconv_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/TorchCNNBuilder/torchcnnbuilder/builder.py:424\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, ascending, conv_dim)\u001B[0m\n\u001B[1;32m    422\u001B[0m input_layer_size \u001B[38;5;241m=\u001B[39m input_layer_size_list[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size) \u001B[38;5;241m-\u001B[39m conv_dim \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 424\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    425\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe difference in dimensions between input_size (input_size.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    426\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand convolution (conv_dim=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconv_dim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) should not be more than 1 \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    427\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(input_size.shape - conv_dim should be equal to 1 or 0)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(torch\u001B[38;5;241m.\u001B[39mtensor(input_layer_size) \u001B[38;5;241m<\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminimum_feature_map_size)[\n\u001B[1;32m    430\u001B[0m                                           :\u001B[38;5;28mlen\u001B[39m(input_layer_size)]):\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput size and parameters can not provide more than \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: The difference in dimensions between input_size (input_size.shape=(31,)) and convolution (conv_dim=3) should not be more than 1 (input_size.shape - conv_dim should be equal to 1 or 0)"
     ]
    }
   ],
   "source": [
    "error_test_builder = Builder(input_size=(31,))\n",
    "error_test_builder.build_convolve_sequence(n_layers=5,\n",
    "                                           in_channels=1,\n",
    "                                           normalization='batchnorm',\n",
    "                                           conv_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c59c88-cb18-44ac-8804-3660305230df",
   "metadata": {},
   "source": [
    "#### Method `build_convolve_block`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fe33d-d567-469e-93ef-a3d01b9f1e5f",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **in_channels**: number of channels in the input image\n",
    "- **out_channels**: number of channels produced by the convolution\n",
    "- **params**: convolutional layer parameters (nn.Conv2d). Default: None\n",
    "- **normalization**: choice of normalization between str `'dropout'` and `'batchnorm'`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for dropout). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for dropout). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for batchnorm). Default: 1e-5\n",
    "- **momentum**: used for the running_mean -_var computation. Can be None for cumulative moving average (for batchnorm). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for batchnorm). Default: True\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential one convolution block with an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319fa24-5e3d-4ff5-b0bc-d084a6315352",
   "metadata": {},
   "source": [
    "If you want a more subtle selection of the hyperparameters of the convolutional layers, it is better to use the logic of convolutional blocks. In each such block, you can configure convolutions and normalization layers *(by default, all parameters are as in `torch`)*, and then combine these blocks into `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e15a9c3-aa5d-49a2-a4ff-96232c022078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:45.305805Z",
     "start_time": "2024-02-09T14:54:45.286183Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "input_image = torch.rand(1, 3, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "699a710c-557f-4d97-9516-9830efe9f407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:46.261535Z",
     "start_time": "2024-02-09T14:54:46.255527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (sub-block 1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (sub-block 2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (sub-block 3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3, \n",
    "                                          out_channels=64, \n",
    "                                          normalization='dropout',\n",
    "                                          p=0.2,\n",
    "                                          sub_blocks=3)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "161ca3ad-3036-4534-b416-a433287f0360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:47.188123Z",
     "start_time": "2024-02-09T14:54:47.172463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 100, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the tensor passes through the block\n",
    "output = conv_layer(input_image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bc6e55-2230-4ec2-a3eb-c76a07dca502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:48.041401Z",
     "start_time": "2024-02-09T14:54:48.026631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), dilation=(3, 3))\n",
       "  (1): BatchNorm2d(64, eps=1e-07, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3, \n",
    "                                          out_channels=64, \n",
    "                                          params={'kernel_size': (7, 7), 'dilation': (3, 3)},\n",
    "                                          normalization='batchnorm',\n",
    "                                          eps=1e-7)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4643625-d9e7-4d79-b156-f9429e8516d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:54:48.831839Z",
     "start_time": "2024-02-09T14:54:48.825566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 82, 82])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the tensor passes through the block\n",
    "output = conv_layer(input_image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ef3595e64ed43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also change the dimension of block layers by using `conv_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51da999757a721a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:11.013041Z",
     "start_time": "2024-02-09T14:55:10.991827Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv3d(3, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (1): Dropout3d(p=0.5, inplace=False)\n",
       "  (2): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_block(in_channels=3,\n",
    "                             out_channels=4,\n",
    "                             normalization='dropout',\n",
    "                             conv_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e649b2f-17d6-4962-8641-567113cd23a2",
   "metadata": {},
   "source": [
    "#### Method `build_transpose_convolve_sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab1942-12ff-44dc-b55e-1a4fff60d2e9",
   "metadata": {},
   "source": [
    "Params: \n",
    "\n",
    "- **n_layers**: number of the convolution layers in the encoder part\n",
    "- **in_channels**: number of channels in the first input tensor. Default: None\n",
    "- **out_channels**: number of channels after the transposed convolution sequence. Default: 1\n",
    "- **out_size**: output size after the transposed convolution sequence. Default: None (input size)\n",
    "- **params**: transposed convolutional layer parameters (nn.ConvTranspose2d). Default: None\n",
    "- **normalization**: choice of normalization between str `'dropout'` and `'batchnorm'`. Default: None\n",
    "- **sub_blocks**: number of transposed convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for dropout). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for dropout). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for batchnorm). Default: 1e-5\n",
    "- **momentum**: used for the running_mean -_var computation. Can be None for cumulative moving average (for batchnorm). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for batchnorm). Default: True\n",
    "- **ratio**: multiplier for the geometric progression of increasing channels (feature maps). Default: 2 (powers of two)\n",
    "- **ascending**: the way of calculating the number of feature maps (with using `'ratio'` if False). Default: False\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential transposed convolutional sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902e299-2c03-4471-92f2-cd91144bfe3e",
   "metadata": {},
   "source": [
    "The method helps to build transposed convolutional sequences without writing the entire code by hand. The activation function is the one that was given at the time of initialization of the entire class. It is possible to use different types of normalization, but to manipulate the hyperparameters of this layer, it is necessary to build a transposed convolutional sequence of transposed convolutional blocks separately *(method `build_transpose_convolve_block`)*. The number of channels (feature maps) after each layer is calculated depending on the parameter `ascending`. See the calculation formulas above in the function, everything is considered the same here, but in the opposite direction.If an activation function was specified during initialization of the class *(see attributes during initialization)* after the last layer, then it will be applied\n",
    "\n",
    "To eliminate the problem of size discrepancy between the output tensor after the decoder part and the input tensor before the encoder part *(due to the parity/dishonesty of the convolution parameters, some pixels may be lost)*, there is a pooling layer at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f25ba7ce-d75f-4c19-a7a6-a83df1c139b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:16.186849Z",
     "start_time": "2024-02-09T14:55:16.171033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv 1): Sequential(\n",
       "    (0): ConvTranspose2d(288, 144, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 2): Sequential(\n",
       "    (0): ConvTranspose2d(144, 72, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 3): Sequential(\n",
       "    (0): ConvTranspose2d(72, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa563e27-8bc0-472a-b6bd-1236c498248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:18.653470Z",
     "start_time": "2024-02-09T14:55:18.634576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv 1): Sequential(\n",
       "    (0): ConvTranspose2d(3, 1, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 2): Sequential(\n",
       "    (0): ConvTranspose2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=2, \n",
    "                                          in_channels=3,\n",
    "                                          params={'kernel_size': 5, 'padding': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a351cf6-93ac-4e9f-8952-5ab675b83b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:19.671259Z",
     "start_time": "2024-02-09T14:55:19.646176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv 1): Sequential(\n",
       "    (0): ConvTranspose2d(10, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.3, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 2): Sequential(\n",
       "    (0): ConvTranspose2d(9, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (resize): AdaptiveAvgPool2d(output_size=(130, 130))\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=2,\n",
    "                                          in_channels=10,\n",
    "                                          out_channels=3,\n",
    "                                          out_size=(130, 130),\n",
    "                                          normalization='dropout',\n",
    "                                          p=0.3,\n",
    "                                          ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5992b7d-7215-4d21-a3a9-5243ac42d946",
   "metadata": {},
   "source": [
    "Sometimes the transposed convolutional layer parameters you select can cause the number of feature maps to degenerate after one of the layers or become smaller than the specified minimum of channels. In this case, you will receive the corresponding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec937bf-95b2-4398-b803-1a904ff62688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:21.674215Z",
     "start_time": "2024-02-09T14:55:21.628581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdduha/anaconda3/envs/TorchCNNBuilder/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There is too few channels. You can not provide less then 1 channel [layer 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_transpose_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mascending\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/TorchCNNBuilder/torchcnnbuilder/builder.py:624\u001B[0m, in \u001B[0;36mBuilder.build_transpose_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, out_channels, out_size, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, ascending, conv_dim)\u001B[0m\n\u001B[1;32m    621\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere is too many channels. Max channels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 624\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThere is too few channels. You can not provide less then 1 channel [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    627\u001B[0m     in_channels \u001B[38;5;241m=\u001B[39m input_channels_count_list[layer]\n",
      "\u001B[0;31mValueError\u001B[0m: There is too few channels. You can not provide less then 1 channel [layer 3]"
     ]
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=6,\n",
    "                                          in_channels=20,\n",
    "                                          ratio=3,\n",
    "                                          ascending=False) # by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2e2f4-b1fc-4e44-bd13-7b81b555ea13",
   "metadata": {},
   "source": [
    "Examples of different final activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e85046-4218-47c8-8fb5-347fa79b7120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:27.281945Z",
     "start_time": "2024-02-09T14:55:27.261086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv 1): Sequential(\n",
       "    (0): ConvTranspose2d(20, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 2): Sequential(\n",
       "    (0): ConvTranspose2d(10, 5, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.5, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 3): Sequential(\n",
       "    (0): ConvTranspose2d(5, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Dropout2d(p=0.5, inplace=False)\n",
       "    (2): Softmax(dim=None)\n",
       "  )\n",
       "  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# redefining the last activation function\n",
    "builder.finish_activation_function = nn.Softmax()\n",
    "\n",
    "builder.build_transpose_convolve_sequence(n_layers=3,\n",
    "                                          in_channels=20,\n",
    "                                          out_channels=3,\n",
    "                                          normalization='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0cbd1b3-7d99-43a7-b65a-7e08ca55b737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:28.306721Z",
     "start_time": "2024-02-09T14:55:28.295014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (deconv 1): Sequential(\n",
       "    (0): ConvTranspose2d(30, 15, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 2): Sequential(\n",
       "    (0): ConvTranspose2d(15, 7, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv 3): Sequential(\n",
       "    (0): ConvTranspose2d(7, 2, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redefining the last activation function\n",
    "builder.finish_activation_function = 'same'\n",
    "\n",
    "builder.build_transpose_convolve_sequence(n_layers=3,\n",
    "                                          in_channels=30,\n",
    "                                          out_channels=2,\n",
    "                                          normalization='batchnorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f238b17-b351-4a66-a8c5-cd63f54e84f4",
   "metadata": {},
   "source": [
    "#### Method `build_transpose_convolve_block`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31953b1-ea98-4e25-b68f-6536002a5d06",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **in_channels**: number of channels in the input image\n",
    "- **out_channels**: number of channels produced by the convolution\n",
    "- **params**: convolutional layer parameters (nn.Conv2d). Default: None\n",
    "- **normalization**: choice of normalization between str `'dropout'` and `'batchnorm'`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed. Default (for dropout): 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place. Default (for dropout): False\n",
    "- **eps**: a value added to the denominator for numerical stability (for batchnorm). Default: 1e-5\n",
    "- **momentum**: used for the running_mean -_var computation. Can be None for cumulative moving average (for batchnorm). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for batchnorm). Default: True\n",
    "- **last_block**: if True there is no activation function after the transposed convolution. Default: False\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential one convolution block with an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3203ca-1aac-4a53-b90b-de6d80ad46bc",
   "metadata": {},
   "source": [
    "If you want a more subtle selection of the hyperparameters of the transposed convolutional layers, it is better to use the logic of transposed convolutional blocks. In each such block, you can configure transposed convolutions and normalization layers *(by default, all parameters are as in `torch`)*, and then combine these blocks into `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ed2f28f-1e92-4a60-a45c-ea7bcc8e8f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:31.552608Z",
     "start_time": "2024-02-09T14:55:31.545982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (transpose sub-block 1): Sequential(\n",
       "    (0): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (transpose sub-block 2): Sequential(\n",
       "    (0): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (transpose sub-block 3): Sequential(\n",
       "    (0): ConvTranspose2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Dropout2d(p=0.2, inplace=False)\n",
       "    (2): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.finish_activation_function = nn.Softmax()\n",
    "deconv_layer = builder.build_transpose_convolve_block(in_channels=3, \n",
    "                                                      out_channels=64, \n",
    "                                                      normalization='dropout',\n",
    "                                                      p=0.2,\n",
    "                                                      sub_blocks=3,\n",
    "                                                      last_block=True)\n",
    "deconv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef4e6ff9-b651-4e20-83f5-f3b367e3c11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:32.127824Z",
     "start_time": "2024-02-09T14:55:32.101088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): BatchNorm2d(3, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deconv_layer = builder.build_transpose_convolve_block(in_channels=64, \n",
    "                                                      out_channels=3,\n",
    "                                                      normalization='batchnorm',\n",
    "                                                      eps=1e-4)\n",
    "deconv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54d3ff-486e-43a6-bb6e-d92dfc3425cd",
   "metadata": {},
   "source": [
    "Let's compare a convolutional block and a transposed convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f05c1a6c-6e3e-4dae-b293-fc9730a55d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:33.775943Z",
     "start_time": "2024-02-09T14:55:33.770411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3,\n",
    "                                          out_channels=64,\n",
    "                                          normalization='batchnorm',\n",
    "                                          eps=1e-4)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6354873e-2105-4d9a-b441-be47bfb11ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:34.122160Z",
     "start_time": "2024-02-09T14:55:34.113927Z"
    }
   },
   "outputs": [],
   "source": [
    "input_image = torch.rand(1, 3, 125, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a84848cc-e713-40f8-aa87-25a3949cac53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:34.474669Z",
     "start_time": "2024-02-09T14:55:34.460445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 123, 123])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_conv = conv_layer(input_image)\n",
    "after_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b020c33f-d01d-4def-9113-7211be50470b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:34.821619Z",
     "start_time": "2024-02-09T14:55:34.798654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 125, 125])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_deconv = deconv_layer(after_conv)\n",
    "after_deconv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1965b-2457-446c-9bea-1cc132f5e1ea",
   "metadata": {},
   "source": [
    "As we can see, the dimensions of the input and final output tensor are the same. Moreover, you can check the history of tensor sizes after conv and transposed conv sequences (as channels history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "396ab7b6-800b-45c3-a341-21b264e3c998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T14:55:36.340388Z",
     "start_time": "2024-02-09T14:55:36.334274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv channels history: [3, 32, 96, 288]\n",
      "Transposed channels history: [30, 15, 7, 2]\n",
      "Conv sizes history: [[125, 125], (123, 123), (121, 121), (119, 119)]\n",
      "Transposed sizes history: [(119, 119), (121, 121), (123, 123), (125, 125)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Conv channels history: {builder.conv_channels}',\n",
    "      f'Transposed channels history: {builder.transpose_conv_channels}',\n",
    "      f'Conv sizes history: {builder.conv_layers}',\n",
    "      f'Transposed sizes history: {builder.transpose_conv_layers}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499602b323fb77d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchCNNBuilder",
   "language": "python",
   "name": "torchcnnbuilder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
