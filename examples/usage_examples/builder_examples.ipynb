{
 "cells": [
  {
   "cell_type": "code",
   "id": "97f53ee3-145e-4cc9-9863-0a9675e74244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T16:58:48.327502Z",
     "start_time": "2025-06-30T16:58:48.314575Z"
    }
   },
   "source": [
    "# set up an absolute path to the project \n",
    "# not needed in case of `pip install`\n",
    "%run -i ../tools/setup_env.py"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "04afc6e9-fc1d-40a4-bcfd-5e4be2abe2a7",
   "metadata": {},
   "source": [
    "## Usage examples of `torchcnnbuilder.builder`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Warning: The following examples contain a demonstration of custom implemented errors that a user may receive when working with the builder components. ",
   "id": "f7abb65999abce60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This submodule contains `Builder` class, which creates convolutional and transposed convolutional sequences",
   "id": "24142880-d5eb-43b4-a5e1-4a772eada5b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `Builder`",
   "id": "bf86c1fb-a42f-4c47-b95e-639824d77773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": "from torchcnnbuilder.builder import Builder",
   "id": "7c6276f3-326b-4f70-859b-5c3ea618fb31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialization params:\n",
    "\n",
    "- **input_size** (Optional[Sequence[int]]): input size of the input tensor. Default: None\n",
    "- **minimum_feature_map_size** (Sequence[int] | int): minimum feature map size. Default: 5\n",
    "- **max_channels** (int): maximum number of layers after any convolution. Default: 512\n",
    "- **min_channels** (int): minimum number of layers after any convolution. Default: 32\n",
    "- **activation_function** (nn.Module): activation function. Default: nn.ReLU(inplace=True)\n",
    "- **finish_activation_function** (Optional[nn.Module] | str): last activation function, can be same as activation_function (str `'same'`). Default: None\n",
    "\n",
    "Other attributes:\n",
    "- **conv_channels** (List[int]): list of output channels after each convolutional layer\n",
    "- **transpose_conv_channels** (List[int]): list of output channels after each transposed convolutional layer\n",
    "- **conv_layers** (List[tuple]): list of output tensor sizes after each convolutional layer\n",
    "- **transpose_conv_layers** (List[tuple]): list of output tensor sizes after each transposed convolutional layer"
   ],
   "id": "0bd4981e-452e-4a48-9047-8f25886c7954"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Suppose we create a CNN-model for a tensor of size 100 by 100, let the smallest feature map after the convolutions be of size 3 by 3",
   "id": "a090bc04-841e-4311-bc7c-508dd6a60354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "builder = Builder(input_size=[125, 125], \n",
    "                  minimum_feature_map_size=3)"
   ],
   "id": "3c0c07ef-28ab-4131-b311-1912db349b98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### `Builder.build_convolve_sequence`",
   "id": "a9b28373-e720-4c3c-93cf-72fe0e909fb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Params:\n",
    "\n",
    "- **n_layers**: number of the convolution layers in the encoder part\n",
    "- **in_channels**: number of channels in the first input tensor. Default: 1\n",
    "- **params**: convolutional layer parameters (`nn.Conv2d`). Default: None\n",
    "- **normalization**: choice of normalization between str `dropout`, `instancenorm` and `batchnorm`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for `dropout`). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for `dropout`). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for `batchnorm`/`instancenorm`). Default: 1e-5\n",
    "- **momentum**: used for the running_mean or running_var computation. Can be None for cumulative moving average (for `batchnorm`/`instancenorm`). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for `batchnorm`/`instancenorm`). Default: True\n",
    "- **ratio**: multiplier for the geometric progression of increasing channels (feature maps). Used for `channel_growth_rate` as `exponential` or `power` . Default: 2 (powers of two)\n",
    "- **start**: start position of a geometric progression in the case of `channel_growth_rate=exponential`. Default: 32\n",
    "- **channel_growth_rate**: the way of calculating the number of feature maps between `exponential`, `proportion`, `power`, `linear` and `constant`. Default: `exponential`\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential convolutional sequence\n",
    "\n",
    "> WARNING: You cannot use this method without `input_size` param for the `Builder` initialization"
   ],
   "id": "b60624ac-7d3f-4e9a-835a-97eb1ecc3b5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The method helps to build convolutional sequences without writing the entire code by hand. The activation function is the one that was given at the time of initialization of the entire class. It is possible to use different types of normalization, but to manipulate the hyperparameters of this layer, it is necessary to build a convolutional sequence of convolutional blocks separately *(method `build_convolve_block`)*. The number of channels (feature maps) after each layer is calculated depending on the parameter `channel_growth_rate`:\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "    channel_i = start \\times ratio^{i}, \\quad i={1...n} & \\text{if } \\text{exponential} \\\\\n",
    "    stop = \\lfloor \\displaystyle\\frac{(input\\_size[0] + input\\_size[1]) * 0.5}{2} \\rfloor + in\\_channels & \\text{if } \\text{proportion} \\\\\n",
    "    step = \\displaystyle\\frac{stop - in\\_channels}{n\\_layers} \\\\ \n",
    "    channels = \\text{range}(in\\_channels, stop, step) \\\\\n",
    "    channel_i = in\\_channels + i, \\quad i={1...n}  & \\text{if } \\text{linear} \\\\\n",
    "    channel_i = (in\\_channels + i)^{ratio} \\quad i={1...n} & \\text{if } \\text{power} \\\\\n",
    "    channel_i = in\\_channels, \\quad i={1...n}  & \\text{if } \\text{constant}\n",
    "\\end{cases}\n",
    "$$\n"
   ],
   "id": "f6c45e9f-69f1-4553-bcdc-f276c06c91f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5,
   "source": "builder.build_convolve_sequence(n_layers=3)",
   "id": "998c5cd2-f504-4c93-9a7a-a4846d202ca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sometimes the convolutional layer parameters you select can cause the tensor to degenerate after one of the layers or become smaller than the specified minimum size of the feature map. In this case, you will receive the corresponding error",
   "id": "eabd48a6-0fa5-4ecb-af55-6708a0e2f641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size and parameters can not provide more than 4 layers.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkernel_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpadding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdilation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\NSS_lab\\TorchCNNBuilder\\torchcnnbuilder\\builder.py:326\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[1;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, channel_growth_rate, conv_dim)\u001B[0m\n\u001B[0;32m    323\u001B[0m input_layer_size \u001B[38;5;241m=\u001B[39m input_layer_size_list[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    325\u001B[0m _validate_difference_in_dimensions(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, conv_dim)\n\u001B[1;32m--> 326\u001B[0m \u001B[43m_validate_available_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_layer_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimum_feature_map_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    327\u001B[0m _validate_max_channels_number(layer, input_channels_count_list, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels)\n\u001B[0;32m    328\u001B[0m _validate_min_channels_number(layer, input_channels_count_list, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_channels)\n",
      "File \u001B[1;32m~\\Documents\\NSS_lab\\TorchCNNBuilder\\torchcnnbuilder\\_validation.py:27\u001B[0m, in \u001B[0;36m_validate_available_layers\u001B[1;34m(layer, input_layer_size, minimum_feature_map_size)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_available_layers\u001B[39m(\n\u001B[0;32m     22\u001B[0m     layer: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     23\u001B[0m     input_layer_size: Tuple[\u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m     24\u001B[0m     minimum_feature_map_size: Union[Sequence[\u001B[38;5;28mint\u001B[39m], \u001B[38;5;28mint\u001B[39m],\n\u001B[0;32m     25\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensor(input_layer_size) \u001B[38;5;241m<\u001B[39m tensor(minimum_feature_map_size)[: \u001B[38;5;28mlen\u001B[39m(input_layer_size)]):\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput size and parameters can not provide more than \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Input size and parameters can not provide more than 4 layers."
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "builder.build_convolve_sequence(n_layers=5, \n",
    "                                params={'kernel_size': 25, 'padding': 1, 'dilation': 2})"
   ],
   "id": "2a40159c-46e4-47df-92d8-ad8c315b45f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalization layers come with standard parameters as in `torch`. To change the parameters of convolutional layers, pass them as a dictionary",
   "id": "7d6e3dfb-8d11-4b32-9cb2-6abed6415ef3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n  (conv 2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": [
    "builder.build_convolve_sequence(n_layers=2, \n",
    "                                in_channels=3,\n",
    "                                params={'kernel_size': 5, 'padding': 1})"
   ],
   "id": "1736f784-9c7b-4cdd-a767-2b67b1599975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv 2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7,
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                normalization='batchnorm',\n",
    "                                eps=1e-3)"
   ],
   "id": "aa31a8f6-6c93-4feb-86db-4b86e35bb219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.1, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (conv 2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.1, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8,
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                normalization='dropout',\n",
    "                                p=0.1)"
   ],
   "id": "e256bf91-e3ab-4c2e-97a4-16c9f24105b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can build a convolutional sequence from convolutional blocks. Each convolutional block is convolutional layers one after another, passing through which the tensor does not change its size. While this is an early functional, there is no point in making a convolutional sequence of such blocks, since the tensor will not change its size after going through the entire sequence, for this it is necessary to add the implementation of pooling layers",
   "id": "9bc7003c-07f2-4424-bc16-62a31e9c01d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (sub-block 1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (2): ReLU(inplace=True)\n    )\n    (sub-block 2): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (conv 2): Sequential(\n    (sub-block 1): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (2): ReLU(inplace=True)\n    )\n    (sub-block 2): Sequential(\n      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n      (2): ReLU(inplace=True)\n    )\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9,
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                sub_blocks=2,\n",
    "                                normalization='instancenorm')"
   ],
   "id": "e967239b-ab3e-4e32-86da-df2213623ce7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "An example of using channel calculation using the `channel_growth_rate=proportion`",
   "id": "4846a107-26fc-48de-aa1a-c96f8e574d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (sub-block 1): Sequential(\n      (0): Conv2d(3, 34, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n      (1): ReLU(inplace=True)\n    )\n    (sub-block 2): Sequential(\n      (0): Conv2d(34, 34, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n      (1): ReLU(inplace=True)\n    )\n  )\n  (conv 2): Sequential(\n    (sub-block 1): Sequential(\n      (0): Conv2d(34, 65, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n      (1): ReLU(inplace=True)\n    )\n    (sub-block 2): Sequential(\n      (0): Conv2d(65, 65, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n      (1): ReLU(inplace=True)\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10,
   "source": [
    "builder.build_convolve_sequence(n_layers=2,\n",
    "                                in_channels=3,\n",
    "                                params={'kernel_size': 9},\n",
    "                                sub_blocks=2, \n",
    "                                channel_growth_rate='proportion')"
   ],
   "id": "516254e8-b799-4239-b73c-804178854376"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To prevent the accidental creation of heavy models, by default there is a certain limit on the number of output channels (feature maps) after the convolutional layer. You can set it when initializing the class (or change it later as an attribute of the class)",
   "id": "950a6696-f599-4bc3-88cd-3cbf5e3c66b7"
  },
  {
   "cell_type": "code",
   "id": "49bbdacb-bbe7-4a23-95e6-62dff9917eb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T16:54:50.379944Z",
     "start_time": "2025-06-30T16:54:50.267944Z"
    }
   },
   "source": [
    "builder.build_convolve_sequence(n_layers=5,\n",
    "                                params={'kernel_size': 9},\n",
    "                                in_channels=3,\n",
    "                                channel_growth_rate='exponential', # by default\n",
    "                                ratio=3)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "There is too many channels [[864]]. Max channels 512 [layer 4].",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkernel_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m9\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mchannel_growth_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mexponential\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# by default\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m                                \u001B[49m\u001B[43mratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\NSS_lab\\TorchCNNBuilder\\torchcnnbuilder\\builder.py:327\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[1;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, channel_growth_rate, conv_dim)\u001B[0m\n\u001B[0;32m    325\u001B[0m _validate_difference_in_dimensions(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, conv_dim)\n\u001B[0;32m    326\u001B[0m _validate_available_layers(layer, input_layer_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminimum_feature_map_size)\n\u001B[1;32m--> 327\u001B[0m \u001B[43m_validate_max_channels_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_channels_count_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_channels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    328\u001B[0m _validate_min_channels_number(layer, input_channels_count_list, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_channels)\n\u001B[0;32m    330\u001B[0m in_channels \u001B[38;5;241m=\u001B[39m input_channels_count_list[layer]\n",
      "File \u001B[1;32m~\\Documents\\NSS_lab\\TorchCNNBuilder\\torchcnnbuilder\\_validation.py:32\u001B[0m, in \u001B[0;36m_validate_max_channels_number\u001B[1;34m(layer, input_channels_count_list, max_channels)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_max_channels_number\u001B[39m(layer: \u001B[38;5;28mint\u001B[39m, input_channels_count_list: List[\u001B[38;5;28mint\u001B[39m], max_channels: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m>\u001B[39m max_channels:\n\u001B[1;32m---> 32\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     33\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere is too many channels [[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_channels_count_list[layer]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]]. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     34\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax channels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_channels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m].\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: There is too many channels [[864]]. Max channels 512 [layer 4]."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "f2fe0685-8a29-4a6f-977f-740ad60ac75b",
   "metadata": {},
   "source": [
    "If you reduce the number of layers in the previous case, then there will be no error"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee73502e-8fe3-4a92-a7b1-af712fbefae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T16:54:54.468945Z",
     "start_time": "2025-06-30T16:54:54.441945Z"
    }
   },
   "source": [
    "builder.build_convolve_sequence(n_layers=3,\n",
    "                                params={'kernel_size': 9},\n",
    "                                in_channels=3,\n",
    "                                channel_growth_rate='power',\n",
    "                                ratio=3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv 1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 2): Sequential(\n",
       "    (0): Conv2d(64, 125, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv 3): Sequential(\n",
       "    (0): Conv2d(125, 216, kernel_size=(9, 9), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "585571f43caa6bef",
   "metadata": {
    "collapsed": false
   },
   "source": "Also, you can do the same by using 1d or 3d convolution layers, however `Builder` should get the same number of dimensions in the `input_size` *(or not less than `len(input_size) - conv_dim`)*. Otherwise, the corresponding error will be thrown. All normalization layers would be replaced with the corresponding N-dimensional methods"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3feb1aecf3438e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:09.810311Z",
     "start_time": "2024-07-26T09:43:09.805249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (conv 1): Sequential(\n    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n    (1): Dropout1d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (conv 2): Sequential(\n    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n    (1): Dropout1d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (conv 3): Sequential(\n    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n    (1): Dropout1d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (conv 4): Sequential(\n    (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n    (1): Dropout1d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (conv 5): Sequential(\n    (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n    (1): Dropout1d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_builder = Builder(input_size=(31,))\n",
    "test_builder.build_convolve_sequence(n_layers=5,\n",
    "                                    in_channels=1,\n",
    "                                    normalization='dropout',\n",
    "                                    conv_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b1ace9fc69b188",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:10.197407Z",
     "start_time": "2024-07-26T09:43:10.173965Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The difference in dimensions between input_size input_size=(31,) and convolution conv_dim=3 should not be more than 1 (input_size.shape - conv_dim should be equal to 1 or 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m error_test_builder \u001B[38;5;241m=\u001B[39m Builder(input_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m31\u001B[39m,))\n\u001B[0;32m----> 2\u001B[0m \u001B[43merror_test_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43mnormalization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minstancenorm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43mconv_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/nss_lab/TorchCNNBuilder/torchcnnbuilder/builder.py:272\u001B[0m, in \u001B[0;36mBuilder.build_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, start, channel_growth_rate, conv_dim)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_layers):\n\u001B[1;32m    270\u001B[0m     input_layer_size \u001B[38;5;241m=\u001B[39m input_layer_size_list[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 272\u001B[0m     \u001B[43m_validate_difference_in_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconv_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m     _validate_available_layers(layer, input_layer_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mminimum_feature_map_size)\n\u001B[1;32m    274\u001B[0m     _validate_max_channels_number(layer, input_channels_count_list, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels)\n",
      "File \u001B[0;32m~/projects/nss_lab/TorchCNNBuilder/torchcnnbuilder/_validation.py:8\u001B[0m, in \u001B[0;36m_validate_difference_in_dimensions\u001B[0;34m(input_size, conv_dim)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_difference_in_dimensions\u001B[39m(input_size: Sequence[\u001B[38;5;28mint\u001B[39m], conv_dim: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(input_size) \u001B[38;5;241m-\u001B[39m conv_dim \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m----> 8\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m      9\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe difference in dimensions between input_size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_size\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand convolution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconv_dim\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m should not be more than 1 \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(input_size.shape - conv_dim should be equal to 1 or 0)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     12\u001B[0m         )\n",
      "\u001B[0;31mValueError\u001B[0m: The difference in dimensions between input_size input_size=(31,) and convolution conv_dim=3 should not be more than 1 (input_size.shape - conv_dim should be equal to 1 or 0)"
     ]
    }
   ],
   "source": [
    "error_test_builder = Builder(input_size=(31,))\n",
    "error_test_builder.build_convolve_sequence(n_layers=5,\n",
    "                                           in_channels=1,\n",
    "                                           normalization='instancenorm',\n",
    "                                           conv_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c59c88-cb18-44ac-8804-3660305230df",
   "metadata": {},
   "source": [
    "#### `Builder.build_convolve_block`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fe33d-d567-469e-93ef-a3d01b9f1e5f",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **in_channels**: number of channels in the input image\n",
    "- **out_channels**: number of channels produced by the convolution\n",
    "- **params**: convolutional layer parameters (`nn.Conv2d`). Default: None\n",
    "- **normalization**: choice of normalization between str `dropout`, `instancenorm` and `batchnorm`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for `dropout`). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for `dropout`). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for `batchnorm`/`instancenorm`). Default: 1e-5\n",
    "- **momentum**: used for the running_mean or running_var computation. Can be None for cumulative moving average (for `batchnorm`/`instancenorm`). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for `batchnorm`/`instancenorm`). Default: True\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential one convolution block with an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319fa24-5e3d-4ff5-b0bc-d084a6315352",
   "metadata": {},
   "source": [
    "If you want a more subtle selection of the hyperparameters of the convolutional layers, it is better to use the logic of convolutional blocks. In each such block, you can configure convolutions and normalization layers *(by default, all parameters are as in `torch`)*, and then combine these blocks into `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e15a9c3-aa5d-49a2-a4ff-96232c022078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:16.776225Z",
     "start_time": "2024-07-26T09:43:16.762226Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "input_image = torch.rand(1, 3, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699a710c-557f-4d97-9516-9830efe9f407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:17.176375Z",
     "start_time": "2024-07-26T09:43:17.161695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (sub-block 1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (sub-block 2): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (sub-block 3): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3, \n",
    "                                          out_channels=64, \n",
    "                                          normalization='dropout',\n",
    "                                          p=0.2,\n",
    "                                          sub_blocks=3)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161ca3ad-3036-4534-b416-a433287f0360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:17.518087Z",
     "start_time": "2024-07-26T09:43:17.504884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 100, 100])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the tensor passes through the block\n",
    "output = conv_layer(input_image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0bc6e55-2230-4ec2-a3eb-c76a07dca502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:17.839387Z",
     "start_time": "2024-07-26T09:43:17.833872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), dilation=(3, 3))\n  (1): InstanceNorm2d(64, eps=1e-07, momentum=0.1, affine=True, track_running_stats=False)\n  (2): ReLU(inplace=True)\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3, \n",
    "                                          out_channels=64, \n",
    "                                          params={'kernel_size': (7, 7), 'dilation': (3, 3)},\n",
    "                                          normalization='instancenorm',\n",
    "                                          eps=1e-7)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4643625-d9e7-4d79-b156-f9429e8516d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:18.172289Z",
     "start_time": "2024-07-26T09:43:18.160591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 82, 82])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that the tensor passes through the block\n",
    "output = conv_layer(input_image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ef3595e64ed43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can also change the dimension of block layers by using `conv_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51da999757a721a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:18.831510Z",
     "start_time": "2024-07-26T09:43:18.820584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Conv3d(3, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n  (1): Dropout3d(p=0.5, inplace=False)\n  (2): ReLU(inplace=True)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_convolve_block(in_channels=3,\n",
    "                             out_channels=4,\n",
    "                             normalization='dropout',\n",
    "                             conv_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e649b2f-17d6-4962-8641-567113cd23a2",
   "metadata": {},
   "source": [
    "#### `Builder.build_transpose_convolve_sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab1942-12ff-44dc-b55e-1a4fff60d2e9",
   "metadata": {},
   "source": [
    "Params: \n",
    "\n",
    "- **n_layers**: number of the convolution layers in the encoder part\n",
    "- **in_channels**: number of channels in the first input tensor. Default: 1\n",
    "- **out_channels**: number of channels after the transposed convolution sequence. Default: 1\n",
    "- **out_size**: output size after the transposed convolution sequence. Default: None (input size)\n",
    "- **params**: convolutional layer parameters (`nn.Conv2d`). Default: None\n",
    "- **normalization**: choice of normalization between str `dropout`, `instancenorm` and `batchnorm`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed (for `dropout`). Default: 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place (for `dropout`). Default: False\n",
    "- **eps**: a value added to the denominator for numerical stability (for `batchnorm`/`instancenorm`). Default: 1e-5\n",
    "- **momentum**: used for the running_mean or running_var computation. Can be None for cumulative moving average (for `batchnorm`/`instancenorm`). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for `batchnorm`/`instancenorm`). Default: True\n",
    "- **ratio**: multiplier for the geometric progression of increasing channels (feature maps). Used for `channel_growth_rate` as `exponential` or `power` . Default: 2 (powers of two)\n",
    "- **start**: start position of a geometric progression in the case of `channel_growth_rate=exponential`. Default: 32\n",
    "- **channel_growth_rate**: the way of calculating the number of feature maps between `exponential`, `proportion`, `power`, `linear` and `constant`. Default: `exponential`\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "- **adaptive_pool**: choice of a last layer as an adaptive pooling between str `avgpool` or `maxpool`. Default: `avgpool`\n",
    "\n",
    "Returns: nn.Sequential transposed convolutional sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902e299-2c03-4471-92f2-cd91144bfe3e",
   "metadata": {},
   "source": [
    "The method helps to build transposed convolutional sequences without writing the entire code by hand. The activation function is the one that was given at the time of initialization of the entire class. It is possible to use different types of normalization, but to manipulate the hyperparameters of this layer, it is necessary to build a transposed convolutional sequence of transposed convolutional blocks separately *(method `build_transpose_convolve_block`)*. The number of channels (feature maps) after each layer is calculated depending on the parameter `channel_growth_rate`. See the calculation formulas above in the function, everything is considered the same here, but in the opposite direction. If an activation function was specified during initialization of the class *(see attributes during initialization)* after the last layer, then it will be applied\n",
    "\n",
    "To eliminate the problem of size discrepancy between the output tensor after the decoder part and the input tensor before the encoder part *(due to the parity/dishonesty of the convolution parameters, some pixels may be lost)*, there is a pooling layer at the end\n",
    "\n",
    "> WARNING: You cannot use this method without `input_size` param for the `Builder` initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f25ba7ce-d75f-4c19-a7a6-a83df1c139b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:20.203406Z",
     "start_time": "2024-07-26T09:43:20.195912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (deconv 1): Sequential(\n    (0): ConvTranspose2d(216, 108, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n  (deconv 2): Sequential(\n    (0): ConvTranspose2d(108, 54, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n  (deconv 3): Sequential(\n    (0): ConvTranspose2d(54, 1, kernel_size=(3, 3), stride=(1, 1))\n  )\n  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa563e27-8bc0-472a-b6bd-1236c498248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:20.551142Z",
     "start_time": "2024-07-26T09:43:20.547555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (deconv 1): Sequential(\n    (0): ConvTranspose2d(3, 1, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n  )\n  (deconv 2): Sequential(\n    (0): ConvTranspose2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n  )\n  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=2, \n",
    "                                          in_channels=3,\n",
    "                                          params={'kernel_size': 5, 'padding': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a351cf6-93ac-4e9f-8952-5ab675b83b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:20.912151Z",
     "start_time": "2024-07-26T09:43:20.903008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (deconv 1): Sequential(\n    (0): ConvTranspose2d(10, 9, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.3, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (deconv 2): Sequential(\n    (0): ConvTranspose2d(9, 3, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.3, inplace=False)\n  )\n  (resize): AdaptiveAvgPool2d(output_size=(130, 130))\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=2,\n",
    "                                          in_channels=10,\n",
    "                                          out_channels=3,\n",
    "                                          out_size=(130, 130),\n",
    "                                          normalization='dropout',\n",
    "                                          p=0.3,\n",
    "                                          channel_growth_rate='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5992b7d-7215-4d21-a3a9-5243ac42d946",
   "metadata": {},
   "source": [
    "Sometimes the transposed convolutional layer parameters you select can cause the number of feature maps to degenerate after one of the layers or become smaller than the specified minimum of channels. In this case, you will receive the corresponding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec937bf-95b2-4398-b803-1a904ff62688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:21.580293Z",
     "start_time": "2024-07-26T09:43:21.543029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdduha/anaconda3/envs/TorchCNNBuilder/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There is too few channels [0]. You can not provide less then 1 channel [layer 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_transpose_convolve_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mchannel_growth_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mexponential\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/nss_lab/TorchCNNBuilder/torchcnnbuilder/builder.py:454\u001B[0m, in \u001B[0;36mBuilder.build_transpose_convolve_sequence\u001B[0;34m(self, n_layers, in_channels, out_channels, out_size, params, normalization, sub_blocks, p, inplace, eps, momentum, affine, ratio, channel_growth_rate, conv_dim, adaptive_pool)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_layers):\n\u001B[1;32m    453\u001B[0m     _validate_max_channels_number(layer, input_channels_count_list, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_channels)\n\u001B[0;32m--> 454\u001B[0m     \u001B[43m_validate_min_channels_number\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_channels_count_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m     in_channels \u001B[38;5;241m=\u001B[39m input_channels_count_list[layer]\n\u001B[1;32m    457\u001B[0m     out_channels \u001B[38;5;241m=\u001B[39m input_channels_count_list[layer \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/projects/nss_lab/TorchCNNBuilder/torchcnnbuilder/_validation.py:34\u001B[0m, in \u001B[0;36m_validate_min_channels_number\u001B[0;34m(layer, input_channels_count_list, min_channels)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_min_channels_number\u001B[39m(layer: \u001B[38;5;28mint\u001B[39m, input_channels_count_list: List[\u001B[38;5;28mint\u001B[39m], min_channels: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m input_channels_count_list[layer] \u001B[38;5;241m<\u001B[39m min_channels:\n\u001B[0;32m---> 34\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     35\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere is too few channels [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_channels_count_list[layer]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     36\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can not provide less then 1 channel [layer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     37\u001B[0m         )\n",
      "\u001B[0;31mValueError\u001B[0m: There is too few channels [0]. You can not provide less then 1 channel [layer 3]"
     ]
    }
   ],
   "source": [
    "builder.build_transpose_convolve_sequence(n_layers=6,\n",
    "                                          in_channels=20,\n",
    "                                          ratio=3,\n",
    "                                          channel_growth_rate='exponential') # by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2e2f4-b1fc-4e44-bd13-7b81b555ea13",
   "metadata": {},
   "source": [
    "Examples of different final activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e85046-4218-47c8-8fb5-347fa79b7120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:22.165728Z",
     "start_time": "2024-07-26T09:43:22.159822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (deconv 1): Sequential(\n    (0): ConvTranspose2d(20, 10, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (deconv 2): Sequential(\n    (0): ConvTranspose2d(10, 5, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.5, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (deconv 3): Sequential(\n    (0): ConvTranspose2d(5, 3, kernel_size=(3, 3), stride=(1, 1))\n    (1): Dropout2d(p=0.5, inplace=False)\n    (2): Softmax(dim=None)\n  )\n  (resize): AdaptiveAvgPool2d(output_size=(125, 125))\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# redefining the last activation function\n",
    "builder.finish_activation_function = nn.Softmax()\n",
    "\n",
    "builder.build_transpose_convolve_sequence(n_layers=3,\n",
    "                                          in_channels=20,\n",
    "                                          out_channels=3,\n",
    "                                          normalization='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0cbd1b3-7d99-43a7-b65a-7e08ca55b737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:22.482085Z",
     "start_time": "2024-07-26T09:43:22.470748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (deconv 1): Sequential(\n    (0): ConvTranspose2d(30, 15, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (deconv 2): Sequential(\n    (0): ConvTranspose2d(15, 7, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (deconv 3): Sequential(\n    (0): ConvTranspose2d(7, 2, kernel_size=(3, 3), stride=(1, 1))\n    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (resize): AdaptiveMaxPool2d(output_size=(125, 125))\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redefining the last activation function\n",
    "builder.finish_activation_function = 'same'\n",
    "\n",
    "builder.build_transpose_convolve_sequence(n_layers=3,\n",
    "                                          in_channels=30,\n",
    "                                          out_channels=2,\n",
    "                                          normalization='batchnorm',\n",
    "                                          adaptive_pool='maxpool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f238b17-b351-4a66-a8c5-cd63f54e84f4",
   "metadata": {},
   "source": [
    "#### `Builder.build_transpose_convolve_block`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31953b1-ea98-4e25-b68f-6536002a5d06",
   "metadata": {},
   "source": [
    "Params:\n",
    "\n",
    "- **in_channels**: number of channels in the input image\n",
    "- **out_channels**: number of channels produced by the convolution\n",
    "- **params**: convolutional layer parameters (nn.Conv2d). Default: None\n",
    "- **normalization**: choice of normalization between str `dropout`, `instancenorm` and `batchnorm`. Default: None\n",
    "- **sub_blocks**: number of convolutions in one layer. Default: 1\n",
    "- **p**: probability of an element to be zero-ed. Default (for `dropout`): 0.5\n",
    "- **inplace**: if set to True, will do this operation in-place. Default (for `dropout`): False\n",
    "- **eps**: a value added to the denominator for numerical stability (for `batchnorm`/`instancenorm`). Default: 1e-5\n",
    "- **momentum**: used for the running_mean or running_var computation. Can be None for cumulative moving average (for `batchnorm`/`instancenorm`). Default: 0.1\n",
    "- **affine**: a boolean value that when set to True, this module has learnable affine parameters (for `batchnorm`/`instancenorm`). Default: True\n",
    "- **last_block**: if True there is no activation function after the transposed convolution. Default: False\n",
    "- **conv_dim**: the dimension of the convolutional operation. Default: 2\n",
    "\n",
    "Returns: nn.Sequential one convolution block with an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3203ca-1aac-4a53-b90b-de6d80ad46bc",
   "metadata": {},
   "source": [
    "If you want a more subtle selection of the hyperparameters of the transposed convolutional layers, it is better to use the logic of transposed convolutional blocks. In each such block, you can configure transposed convolutions and normalization layers *(by default, all parameters are as in `torch`)*, and then combine these blocks into `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed2f28f-1e92-4a60-a45c-ea7bcc8e8f7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:23.814374Z",
     "start_time": "2024-07-26T09:43:23.799018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (transpose sub-block 1): Sequential(\n    (0): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (transpose sub-block 2): Sequential(\n    (0): ConvTranspose2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): ReLU(inplace=True)\n  )\n  (transpose sub-block 3): Sequential(\n    (0): ConvTranspose2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Dropout2d(p=0.2, inplace=False)\n    (2): Softmax(dim=None)\n  )\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.finish_activation_function = nn.Softmax()\n",
    "deconv_layer = builder.build_transpose_convolve_block(in_channels=3, \n",
    "                                                      out_channels=64, \n",
    "                                                      normalization='dropout',\n",
    "                                                      p=0.2,\n",
    "                                                      sub_blocks=3,\n",
    "                                                      last_block=True)\n",
    "deconv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef4e6ff9-b651-4e20-83f5-f3b367e3c11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:24.465759Z",
     "start_time": "2024-07-26T09:43:24.443282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): ConvTranspose2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n  (1): BatchNorm2d(3, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deconv_layer = builder.build_transpose_convolve_block(in_channels=64, \n",
    "                                                      out_channels=3,\n",
    "                                                      normalization='batchnorm',\n",
    "                                                      eps=1e-4)\n",
    "deconv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Attributes examples and blocks comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2cadc7b2d45cc86"
  },
  {
   "cell_type": "markdown",
   "id": "be54d3ff-486e-43a6-bb6e-d92dfc3425cd",
   "metadata": {},
   "source": [
    "Let's compare a convolutional block and a transposed convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f05c1a6c-6e3e-4dae-b293-fc9730a55d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:25.601047Z",
     "start_time": "2024-07-26T09:43:25.587171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n  (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = builder.build_convolve_block(in_channels=3,\n",
    "                                          out_channels=64,\n",
    "                                          normalization='batchnorm',\n",
    "                                          eps=1e-4)\n",
    "conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6354873e-2105-4d9a-b441-be47bfb11ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:26.107469Z",
     "start_time": "2024-07-26T09:43:26.096780Z"
    }
   },
   "outputs": [],
   "source": [
    "input_image = torch.rand(1, 3, 125, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a84848cc-e713-40f8-aa87-25a3949cac53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:26.421567Z",
     "start_time": "2024-07-26T09:43:26.410069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 123, 123])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_conv = conv_layer(input_image)\n",
    "after_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b020c33f-d01d-4def-9113-7211be50470b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:26.767882Z",
     "start_time": "2024-07-26T09:43:26.756050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 125, 125])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_deconv = deconv_layer(after_conv)\n",
    "after_deconv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1965b-2457-446c-9bea-1cc132f5e1ea",
   "metadata": {},
   "source": [
    "As we can see, the dimensions of the input and final output tensor are the same. Moreover, you can check the history of tensor sizes after conv and transposed conv sequences (as channels history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "396ab7b6-800b-45c3-a341-21b264e3c998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T09:43:28.348966Z",
     "start_time": "2024-07-26T09:43:28.343982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv channels history: [3, 64, 125, 216]\n",
      "Transposed channels history: [30, 15, 7, 2]\n",
      "Conv sizes history: [(125, 125), (123, 123), (121, 121), (119, 119)]\n",
      "Transposed sizes history: [(119, 119), (121, 121), (123, 123), (125, 125)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Conv channels history: {builder.conv_channels}',\n",
    "      f'Transposed channels history: {builder.transpose_conv_channels}',\n",
    "      f'Conv sizes history: {builder.conv_layers}',\n",
    "      f'Transposed sizes history: {builder.transpose_conv_layers}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea95e87-3dff-41de-9362-a6c5ee9f308e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
